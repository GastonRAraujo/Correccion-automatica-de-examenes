{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlcFOMNo5FPT"
   },
   "source": [
    "#Corrección automática de examenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK7bLCJW5HCC"
   },
   "source": [
    "En el presente trabajo se utilizará un dataset que consta de ocho grupos de \"ensayos\" con una extensión promedio de 150 a 550 palabras. El objetivo es aprender a utilizar texto como variable de entrada, el cual requiere una etapa de preprocesado diferente al utilizar otro tipo de variables (numéricas, categoricas, etc...). Esto se llevará a cabo implementando diferentes modelos de clasificación que permitan asignar una calificacion a cada \"ensayo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTwQ16zI5Q25"
   },
   "source": [
    "##Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYqE8ORLJYb9"
   },
   "source": [
    "## Cargar dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19hX5uUzD7k9"
   },
   "source": [
    "Para cargar el dataset, utilizo la API de Kaggle. La cual requiere utilizar un archivo \".json\" que permite descargar el dataset. Luego subí los archivos al repostiorio del proyecto para evitar repetir este proceso (subir .json y descargar todo el dataset) cada vez que utilice colab. \n",
    "Para descargar los datos cree el notebook [DownloadData_wAPI](https://github.com/GastonRAraujo/Materia-Ap_Maq/blob/master/Proyecto_Final/DownloadData_wAPI.ipynb), donde se encuentra el procedimiento paso a paso para descargar los archivos y subirlos a GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WvjJa13DWr0"
   },
   "source": [
    "Una vez realizado esto, podemos acceder a ellos utilizando el link que nos otorga GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2922,
     "status": "ok",
     "timestamp": 1614016328614,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "iIwyeJiMMj01"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'set_option'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-74bf3695e581>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_rows'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://raw.githubusercontent.com/GastonRAraujo/Materia-Ap_Maq/master/Proyecto_Final/training_set_rel3.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ISO-8859-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://raw.githubusercontent.com/GastonRAraujo/Materia-Ap_Maq/master/Proyecto_Final/valid_set.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ISO-8859-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'set_option'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 4)\n",
    "\n",
    "df_train = pd.read_csv('https://raw.githubusercontent.com/GastonRAraujo/Materia-Ap_Maq/master/Proyecto_Final/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
    "df_test = pd.read_csv('https://raw.githubusercontent.com/GastonRAraujo/Materia-Ap_Maq/master/Proyecto_Final/valid_set.csv', sep=',', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 2920,
     "status": "ok",
     "timestamp": 1614016328618,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "ooQDnzVPOdQE",
    "outputId": "9a3d6b6d-868d-4e5e-dccd-69a9294c0642"
   },
   "outputs": [],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 2915,
     "status": "ok",
     "timestamp": 1614016328619,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "tWIsyzZPOa5U",
    "outputId": "6b96150f-5c88-40f2-d79c-a598838cf321"
   },
   "outputs": [],
   "source": [
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2909,
     "status": "ok",
     "timestamp": 1614016328619,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "loofQJw3OrUc",
    "outputId": "e82c4119-dfe7-42f4-8c49-3f6f11175250"
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2905,
     "status": "ok",
     "timestamp": 1614016328620,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "JqqCFOuhTnnV",
    "outputId": "7a116743-67d1-4873-b670-29a1a434a532"
   },
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2898,
     "status": "ok",
     "timestamp": 1614016328620,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "tm96dmjpXRNI",
    "outputId": "bfe6da47-fdc5-4e36-995f-2ab6c7ba07e4"
   },
   "outputs": [],
   "source": [
    "print(df_train.groupby('essay_set')['essay_id'].nunique().values)\n",
    "\n",
    "print(df_test.groupby('essay_set')['essay_id'].nunique().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3UgXQ2yX7o_"
   },
   "source": [
    "En ambos casos el grupo 8 se ve subrepresentado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMyrJJnnO12U"
   },
   "source": [
    "Oservamos una gran presencia de valores null en algunas columnas, si investigamos sobre el dataset podemos ver el significado de cada una y poque sucede esto:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5pmybUHSK9s"
   },
   "source": [
    "  * essay_id: un identificador único para cada ensayo individual del estudiante\n",
    "\n",
    "  * essay_set: 1-8, una identificación para cada conjunto de ensayos\n",
    "\n",
    "  * essay: el texto ascii de la respuesta de un estudiante\n",
    "\n",
    "  * rater1_domain1: puntuación del dominio 1 del evaluador 1; todos los ensayos tienen esto\n",
    "\n",
    "  * rater2_domain1: puntuación del dominio 1 del evaluador 2; todos los ensayos tienen esto\n",
    "\n",
    "  * rater3_domain1: puntuación del dominio 1 del evaluador 3; solo algunos ensayos del conjunto 8 tienen esto.\n",
    "\n",
    "  * domain1_score: puntuación resuelta entre los evaluadores; todos los ensayos tienen esto\n",
    "\n",
    "  * rater1_domain2: puntuación del dominio 2 del evaluador 1; solo los ensayos del conjunto 2 tienen esto\n",
    "\n",
    "  * rater2_domain2: puntuación del dominio 2 del evaluador 2; solo los ensayos del conjunto 2 tienen esto\n",
    "\n",
    "  * domain2_score: puntuación resuelta entre los evaluadores; solo los ensayos del conjunto 2 tienen esto\n",
    "\n",
    "  * rater1_trait1 score - rater3_trait6 score: puntajes de rasgos para los conjuntos 7-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJgE8r775WHx"
   },
   "source": [
    "##Procesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZG0aZF6FT-k"
   },
   "source": [
    "En esta sección procesaremos los datos por lo que analizaremos como reducir la dimension de nuestro dataset, como tratar los valores NaN y por último realizaremos el procesamiento de texto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rH3Ro8ULGGaE"
   },
   "source": [
    "###Procesamiento de variables númericas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBAguMy5YZW3"
   },
   "source": [
    "Analicemos la distribucion de notas de cada grupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 4048,
     "status": "ok",
     "timestamp": 1614016329780,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "cj7M7LpxVO7m",
    "outputId": "7c32f5cb-f8a0-4da3-f307-35b595cb71cd"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.histogram(df_train, x=\"domain1_score\", color=\"essay_set\", marginal=\"rug\", hover_data=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAuDRcdnY_Cf"
   },
   "source": [
    "Los grupos poseen diferentes distribuciones de notas. Mientras la mayoría se encuentra con una media menor a 30 puntos, el grupo 8 posee una distribución con notas más altas.\n",
    "A su vez se observa que una gran cantidad de alumnos a obtenido una calificación menor a 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 655
    },
    "executionInfo": {
     "elapsed": 5378,
     "status": "ok",
     "timestamp": 1614016331119,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "68yuhrCkEFPG",
    "outputId": "52500c06-ef82-448d-c968-09b23ce9d3b2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Correlation matrix\n",
    "plt.figure(dpi=150)\n",
    " \n",
    "sns.heatmap(df_train.corr(), vmin=-1, vmax=1, center= 0, xticklabels=True, yticklabels=True)\n",
    " \n",
    "#Adjust font size\n",
    "sns.set(font_scale=0.7)\n",
    " \n",
    "# Rotate the tick labels a bit\n",
    "plt.xticks(rotation=85)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v2NQzYet86z"
   },
   "source": [
    "Se observa una correlación entre algunas columnas, lo cual surge de que las columnas \"rater#_trait#\" contienen la información con la que cada evualdor calificó finalmente al alumno. Por lo que un alumno que obtenga altos puntajes en los diferentes rasgos, obtendrá un puntaje alto según ese evaluador. A su vez vemos que estos rasgos estan fuertemente relacionados entre si.\n",
    "En principio pareciera que \"domain1_score\" es simplemente la suma de las calificaciones. Comprobemos que \"domain1_score\" NO es la suma de las calificaciones que le han otorgado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "executionInfo": {
     "elapsed": 5371,
     "status": "ok",
     "timestamp": 1614016331120,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "lx8ouzg45pFD",
    "outputId": "a72494a8-aed1-40e6-813f-459fff4cffe8"
   },
   "outputs": [],
   "source": [
    "dummy_df = df_train[df_train['domain1_score']!=df_train['rater1_domain1']+df_train['rater2_domain1']]\n",
    "\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "executionInfo": {
     "elapsed": 5363,
     "status": "ok",
     "timestamp": 1614016331120,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "kTmRzlzdswi5",
    "outputId": "002942c9-d17d-4527-ce9a-81bd60b5390c"
   },
   "outputs": [],
   "source": [
    "dummy_df = df_train[df_train['rater3_domain1']!=df_train['domain1_score']]\n",
    "\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyR0c4Ugqfj8"
   },
   "source": [
    "Por lo que la columna \"domain1_score\" no es la suma ni el promedio de las columnas \"rater1_domain1\",\t\"rater2_domain1\" y\t\"rater3_domain1\". Hay una similitud en \"domain1_score\" y \"rater3_domain1\" por lo que descartaremos esta última, además de que la amplia mayoría no cuenta con un valor para esta característica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "executionInfo": {
     "elapsed": 5355,
     "status": "ok",
     "timestamp": 1614016331120,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "HW_AJeLzweyH",
    "outputId": "210c0d4e-6ec9-43c9-8c18-da3b378c5099"
   },
   "outputs": [],
   "source": [
    "dummy_df = df_train[df_train['rater1_trait1']>0]\n",
    "\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huIxGDhGx32X"
   },
   "source": [
    "Las columnas \"rater#_trait#\" nos dan información de como obtuvieron el puntaje de \"rater#_domain1\" por lo que no las utilizaremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHLkWH36y7S_"
   },
   "source": [
    "Repitamos el analisis pero para el \"domain2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "executionInfo": {
     "elapsed": 5348,
     "status": "ok",
     "timestamp": 1614016331121,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "7dBaOplouZnr",
    "outputId": "bc17330f-bc93-4a9a-a86e-4486f4b80c34"
   },
   "outputs": [],
   "source": [
    "dummy_df = df_train[df_train['domain2_score']>0]\n",
    "\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QupX5SANzZFa"
   },
   "source": [
    "Observamos que nuevamente la columna \"domain1_score\" no corresponde a la suma ni al promedio de las columnas de \"rater#_domain2\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrAr6XoXD2ef"
   },
   "source": [
    "A continuacion eliminamos las columnas que no utilizaremos y unificaremos ambos puntajes \"domain#_score\" realizando un promedio de ambos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 5340,
     "status": "ok",
     "timestamp": 1614016331121,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "Jm3ndyMSp6ao",
    "outputId": "a9523f0c-b30c-4a25-f029-78acc950557a"
   },
   "outputs": [],
   "source": [
    "# Calculamos el promedio de puntajes ignorando NaN\n",
    "# Es decir que el promedio de 5 y Nan es 5, pero 5 y 2 es (5+2)/2\n",
    "df_train['score'] = df_train[['domain1_score', 'domain2_score']].mean(axis=1)\n",
    "\n",
    "\n",
    "# Eliminamos las columnas que no utilizaremos:\n",
    "\n",
    "#Eliminamos aquellas que tengan valores NaN\n",
    "df_train.dropna(axis='columns',inplace=True)\n",
    "\n",
    "#Eliminamos las restantes\n",
    "df_train.drop(['rater1_domain1',\t'rater2_domain1',\t'domain1_score'], axis=1, inplace=True)\n",
    "\n",
    "fig = px.histogram(df_train, x=\"score\", color=\"essay_set\", histnorm='probability density')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 5721,
     "status": "ok",
     "timestamp": 1614016331511,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "J4q_xejiVNhP",
    "outputId": "2ebe1256-776c-4fb8-8647-136d3749d7d6"
   },
   "outputs": [],
   "source": [
    "df_train.drop(['essay_id',\t'essay_set'], axis=1, inplace=True)\n",
    "\n",
    "fig = px.histogram(df_train, x=\"score\", histnorm='probability density')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2nqZMhpfNkT"
   },
   "source": [
    "Observamos la misma distribución que al comienzo solo que los grupos han sido unificados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA0Zqf4_II-G"
   },
   "source": [
    "Para los datos de validación solo hace falta eliminar las columnas correspondientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "executionInfo": {
     "elapsed": 5714,
     "status": "ok",
     "timestamp": 1614016331512,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "UWZByzLbIN4m",
    "outputId": "8b800fae-eef4-4911-bb3b-6911c97de366"
   },
   "outputs": [],
   "source": [
    "# Eliminamos las restantes\n",
    "df_test.drop(['essay_id',\t'essay_set'], axis=1, inplace=True)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 5706,
     "status": "ok",
     "timestamp": 1614016331512,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "JCkdi67RmfCA",
    "outputId": "4bf3229f-5283-4823-b0ad-0f8dd085c20a"
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(df_test, x=\"predicted_score\", histnorm='probability density')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T46IqZtdyTdP"
   },
   "source": [
    "###Procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUFg8NivCrPP"
   },
   "source": [
    "Para procesar el texto y poder usarlo como input de nuestro modelo debemos realizar los siguientes pasos:\n",
    "\n",
    "* *Tokenizar* el texto: convertir el texto en lista de palabras\n",
    "* Cambiar mayúsculas por minúsculas\n",
    "* Remover *stop words* (palabras que no aportan información: the, is, at, which, on, for, this, etc.) y signos de punctuación\n",
    "* Stemming: método para reducir una palabra a su raíz \n",
    "* Lematización: proceso lingüístico que consiste en, dada una forma flexionada (es decir, en plural, en femenino, conjugada, etc), hallar el lema correspondiente. Por ejemplo decir es el lema de dije\n",
    "* Vectorización: convertir nuestro texto tokenizado y procesado en vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xHiqzX_9NHZ"
   },
   "source": [
    "####Tokenizacion y prepocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5699,
     "status": "ok",
     "timestamp": 1614016331513,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "aYlqpu5PoNNR",
    "outputId": "f984f5fd-c4b9-4ed2-d689-b2c0d2f9d90c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re # expresiones regulares\n",
    "import nltk #procesar texto\n",
    "\n",
    "#descargas necesarias\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Procesamiento de texto\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet #tipo de palabra\n",
    "from nltk import word_tokenize #tokenizer\n",
    "\n",
    "import string  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIcn1GRvjmb-"
   },
   "source": [
    "La siguiente función elimina las signos de puntuación, convierte mayúsculas, remueve URLS, tokeniza por minúsculas y por último aplica un stemmer que reduce las palabras a su raiz. Esto puede generar que algunas palabras sean cortadas o pierdan caracteres por lo que luego se aplica un lematizador que "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5698,
     "status": "ok",
     "timestamp": 1614016331513,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "zLd0N6rSmAzh"
   },
   "outputs": [],
   "source": [
    "# Stemmer \n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# creo una lista de signos de puntuación que serán eliminados\n",
    "Punct_Sign = string.punctuation\n",
    "\n",
    "# Set de stopwords del idioma ingles\n",
    "stopwords_english = set(stopwords.words('english'))\n",
    "\n",
    "# Utilizo expresiones regulares para quitar URLs\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Cambiar Mayúscula por minúscula\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remuevo signos de puntuación\n",
    "    # maketrans(x,y,z) cambia \"x\" por \"y\" y remueve \"z\"\n",
    "    text = text.translate(str.maketrans('', '', Punct_Sign)) #no hago cambios pero remuevo signos de puntuacion\n",
    "\n",
    "    # Remuevo links\n",
    "    text = url_pattern.sub(r'', text)\n",
    "\n",
    "    # StopWords\n",
    "    text = [stemmer.stem(word) for word in str(text).split() if word not in stopwords_english]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "executionInfo": {
     "elapsed": 34438,
     "status": "ok",
     "timestamp": 1614016360261,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "cY_5XiwBk1ZI",
    "outputId": "98a06872-e7a7-4584-9507-f34e6ec35e83"
   },
   "outputs": [],
   "source": [
    "df_train[\"text\"] = df_train[\"essay\"].apply(lambda text: clean_text(text))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EtBMGewlOPW"
   },
   "source": [
    "El stemmer puede suceder que elimine letras o que no encuentre correctamente la raiz como en el caso de \"having\". Esto se debe a que se centra en ser un algoritmo rápido pero que no tiene en cuenta el contexto ni utiliza una base de datos de léxico, no podría diferenciar dos palabras con significados similares/distintos como el caso de \"better\" que posee \"good\" como lema pero \"bet\" como raiz. Se perdería la conexión entre ambas entidades. Por lo que cambiaremos el stemmer por el lematizador, al cual hay que indicarle el tipo de palabra que debe procesar debido a que una misma palabra puede ser un verbo o un sustantivo y poseer diferente raiz según el caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34437,
     "status": "ok",
     "timestamp": 1614016360262,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "b7NV2fb4lsBH"
   },
   "outputs": [],
   "source": [
    "# Lemmatizer \n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Dic con KeyWord: tipo de palabra\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "\n",
    "# creo una lista de signos de puntuación que serán eliminados\n",
    "Punct_Sign = string.punctuation\n",
    "\n",
    "# Set de stopwords del idioma ingles\n",
    "stopwords_english = set(stopwords.words('english'))\n",
    "\n",
    "# Utilizo expresiones regulares para quitar URLs\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Cambiar Mayúscula por minúscula\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remuevo signos de puntuación\n",
    "    # maketrans(x,y,z) cambia \"x\" por \"y\" y remueve \"z\"\n",
    "    text = text.translate(str.maketrans('', '', Punct_Sign)) #no hago cambios pero remuevo signos de puntuacion\n",
    "\n",
    "    # Remuevo links\n",
    "    text = url_pattern.sub(r'', text)\n",
    "      \n",
    "    # StopWords\n",
    "    text = [word for word in str(text).split() if word not in stopwords_english]\n",
    "    \n",
    "    #   # Lematización\n",
    "    pos_tagged_text = nltk.pos_tag(text)\n",
    "    text = \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlpQb3Fx4DjX"
   },
   "outputs": [],
   "source": [
    "!pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plRPG5xDyVax"
   },
   "outputs": [],
   "source": [
    "import swifter\n",
    "\n",
    "df_train[\"text\"] = df_train[\"essay\"].swifter.apply(lambda text: clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qh0wCkShmeF3"
   },
   "outputs": [],
   "source": [
    "%time df_train[\"text\"] = df_train[\"essay\"].apply(lambda text: clean_text(text))\n",
    "df_train.drop(columns=\"essay\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2CY4sU2xjBC"
   },
   "outputs": [],
   "source": [
    "df_test[\"text\"] = df_test[\"essay\"].apply(lambda text: clean_text(text))\n",
    "df_test.drop(columns=\"essay\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMf4mUdTnan3"
   },
   "source": [
    "Si bien el proceso es más lento, en este caso particular, obtenemos un mejor resultado con el lematizador. Se debe utilizar el procedimiento correspondiente según los textos a procesar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOsK6sb8u-gC"
   },
   "source": [
    "En algunos casos es conveniente eliminar las palabras más comunes si estas no aportan información útil. Para eso debemos comprobar que no eliminaremos paralbras que si aporten información útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikaiPRPUraqR"
   },
   "outputs": [],
   "source": [
    "#Palabras más comunes\n",
    "from collections import Counter\n",
    "\n",
    "#Contador para datos de entrenamiento\n",
    "cnt = Counter()\n",
    "for text in df_train[\"text\"].values:\n",
    "    for word in text:\n",
    "        cnt[word] += 1\n",
    "\n",
    "print(cnt.most_common(10))\n",
    "\n",
    "#Contador para datos de validación\n",
    "cnt2 = Counter()\n",
    "for text in df_test[\"text\"].values:\n",
    "    for word in text:\n",
    "        cnt2[word] += 1\n",
    "\n",
    "print(cnt2.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLwfAcoDyDzT"
   },
   "source": [
    "En ambos conjutos de datos (validación y entrenamiento), las palabras más comunes coinciden en su mayoría. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqlxtJlkspzw"
   },
   "source": [
    "Al igual que con las palabras más comunes, podemos realizar el mismo analisis para las palabras más raras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71TJ9KVkso3-"
   },
   "outputs": [],
   "source": [
    "n_rare_words = 10\n",
    "rarewords = set([(w,wc) for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "print(rarewords)\n",
    "\n",
    "rarewords2 = set([(w,wc) for (w, wc) in cnt2.most_common()[:-n_rare_words-1:-1]])\n",
    "print(rarewords2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFrHfrSx4Esh"
   },
   "source": [
    "Podemos observar que no solo son palabras que no debemos eliminar sino que no coinciden en ambos casos. Podemos realizar una nube de palabras para ambos conjuntos de datos y así poder comparar las palabras más y menos frecuentes de ambos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34397,
     "status": "aborted",
     "timestamp": 1614016360264,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "bYAAvr1dAcKR"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "a = [' '.join(map(str, l)) for l in df_train['text']]\n",
    "\n",
    "word_cloud = WordCloud(\n",
    "                       width=1600,\n",
    "                       height=800,\n",
    "                       background_color=\"white\",collocations=False\n",
    "            ).generate(\" \".join(a))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(word_cloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34395,
     "status": "aborted",
     "timestamp": 1614016360265,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "gIcfE1B5ABzj"
   },
   "outputs": [],
   "source": [
    "a = [' '.join(map(str, l)) for l in df_test['text']]\n",
    "\n",
    "word_cloud = WordCloud(\n",
    "                       width=1600,\n",
    "                       height=800,\n",
    "                       margin=0,\n",
    "                       background_color=\"white\",collocations=False\n",
    "            ).generate(\" \".join(a))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(word_cloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgpARIvA7ssT"
   },
   "source": [
    "####Vectorización\n",
    "Por último convertiremos el texto ya procesado y tokenizado en vector. Para ello utilizaremos el método [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de scikit-learn. El cual construye una transformación de tokens a vectores a partir del vocabulario de nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34394,
     "status": "aborted",
     "timestamp": 1614016360266,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "3eVYThto_Rgq"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Creo una lista con todos los textos de entrenamiento\n",
    "text_train    = [''.join(x) for x in df_train[\"text\"].values ] # train texts\n",
    "# Creo una lista con todos los textos de validación\n",
    "text_test    = [''.join(x) for x in df_test[\"text\"].values ] # val texts\n",
    "\n",
    "# Unifico ambas para crear una lista con todos los textos\n",
    "# para entrenar el vectorizador\n",
    "texts = text_train + text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34392,
     "status": "aborted",
     "timestamp": 1614016360266,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "zjelJB5UlYqC"
   },
   "outputs": [],
   "source": [
    "# Vectorizador\n",
    "vectorizer = CountVectorizer() \n",
    "vectorizer = vectorizer.fit(texts) #utilizo todos los textos\n",
    "print(\"Vocabulary size:{}\".format(len(vectorizer.vocabulary_)))\n",
    "print(\"Vocabulary content:{}\".format(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34391,
     "status": "aborted",
     "timestamp": 1614016360266,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "fZkoTLa45irC"
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(df_train['text'])\n",
    "y_train = df_train['score']\n",
    "\n",
    "X_test = vectorizer.transform(df_test['text'])\n",
    "y_test = df_test['predicted_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34390,
     "status": "aborted",
     "timestamp": 1614016360267,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "l2msuTvGskdX"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "train_tfidf = tfidf.fit_transform(df_train['text'])\n",
    "test_tfidf = tfidf.transform(df_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34388,
     "status": "aborted",
     "timestamp": 1614016360267,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "GID_WS6msp1A"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34388,
     "status": "aborted",
     "timestamp": 1614016360268,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "T3kc1UL5sePX"
   },
   "outputs": [],
   "source": [
    "# Let's implement simple classifiers\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(n_neighbors=1),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"MultinimialNB\": MultinomialNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34386,
     "status": "aborted",
     "timestamp": 1614016360268,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "my2TYAdYsazf"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "kappa = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "classifier.fit(X_train, df_train[\"score\"].astype(int))\n",
    "training_score = cross_val_score(classifier, X_train, df_train[\"score\"].astype(int), cv=5, scoring=kappa )\n",
    "print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34386,
     "status": "aborted",
     "timestamp": 1614016360269,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "DzbaG6x0xBHg"
   },
   "outputs": [],
   "source": [
    "# Using the Logistic Regression\n",
    "\n",
    "classifier2 = LogisticRegression()\n",
    "\n",
    "classifier2.fit(X_train, df_train[\"score\"].astype(int))\n",
    "training_score = cross_val_score(classifier2, X_train, df_train[\"score\"].astype(int), cv=5, scoring=kappa)\n",
    "print(\"Classifiers: \", classifier2.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34384,
     "status": "aborted",
     "timestamp": 1614016360269,
     "user": {
      "displayName": "Gaston Rodriguez Araujo",
      "photoUrl": "",
      "userId": "14852371452812350832"
     },
     "user_tz": 180
    },
    "id": "i0jQZiwnxKy4"
   },
   "outputs": [],
   "source": [
    "# Using the XGBoost\n",
    "\n",
    "import xgboost as xgb\n",
    "clf_xgb = xgb.XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=3000,\n",
    "    max_depth=15,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='multi:softmax',\n",
    "    nthread=42,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf_xgb, X_train, df_train[\"score\"].astype(int), cv=5, scoring=kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXQFjdd35cJZ"
   },
   "source": [
    "##Modelos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28JR7RP-4zM6"
   },
   "source": [
    "Se entrenarán varios modelos de clasificadores desde los más simples: Naive Bayes y Kmeans hasta modelos más complejos de *DeepLearning* utilisando una red neuronal secuencial con capas LTSM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-5t7nLt5hQ5"
   },
   "source": [
    "##resultados y conclusiones"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOa9ZRQBxhqpNtwrgPdSqtZ",
   "collapsed_sections": [
    "VTwQ16zI5Q25",
    "X-5t7nLt5hQ5"
   ],
   "name": "Main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
